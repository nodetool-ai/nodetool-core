services:
  api:
    image: nodetool:latest
    restart: unless-stopped
    build:
      context: .
      dockerfile: Dockerfile
      args:
        USE_LOCAL_WHEELS: ${USE_LOCAL_WHEELS:-0}
    volumes:
    - ${HF_HOME}:/app/huggingface
    - /var/run/docker.sock:/var/run/docker.sock
    - ${CHROMA_PATH}:/chroma-data
    command: [nodetool, serve, --host, 0.0.0.0, --port, '8000']
    expose:
    - '8000'
    environment:
      ENV: ${ENV:-development}
      JOB_EXECUTION_STRATEGY: ${JOB_EXECUTION_STRATEGY:-threaded}
      ASSET_BUCKET: ${ASSET_BUCKET}
      ASSET_DOMAIN: ${ASSET_DOMAIN}
      ASSET_TEMP_BUCKET: ${ASSET_TEMP_BUCKET}
      ASSET_TEMP_DOMAIN: ${ASSET_TEMP_DOMAIN}
      BRIGHTDATA_API_KEY: ${BRIGHTDATA_API_KEY}
      BRIGHTDATA_SERP_ZONE: ${BRIGHTDATA_SERP_ZONE}
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      S3_ACCESS_KEY_ID: ${S3_ACCESS_KEY_ID}
      S3_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY}
      S3_REGION: ${S3_REGION}
      SUPABASE_URL: ${SUPABASE_URL}
      SUPABASE_KEY: ${SUPABASE_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      VLLM_BASE_URL: ${VLLM_BASE_URL:-http://vllm:8888}
      VLLM_API_KEY: ${VLLM_API_KEY:-}
      VLLM_HTTP_TIMEOUT: ${VLLM_HTTP_TIMEOUT:-600}
      VLLM_VERIFY_TLS: ${VLLM_VERIFY_TLS:-0}
      VLLM_CONTEXT_WINDOW: ${VLLM_CONTEXT_WINDOW:-128000}
      HF_HOME: /app/huggingface
      SENTRY_DSN: ${SENTRY_DSN}
      SECRETS_MASTER_KEY: ${SECRETS_MASTER_KEY}
      MEMCACHE_HOST: ${MEMCACHE_HOST}
      MEMCACHE_PORT: ${MEMCACHE_PORT}
      NODETOOL_API_URL: ${NODETOOL_API_URL}
      CHROMA_PATH: /chroma-data
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
  memcached:
    image: memcached:bookworm
    restart: unless-stopped

  vllm:
    image: ${VLLM_IMAGE:-vllm/vllm-openai:latest}
    restart: unless-stopped
    ipc: host
    environment:
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN:-}
    volumes:
    - ${HF_HOME}:/root/.cache/huggingface
    - ${VLLM_CACHE_ROOT}:/root/.cache/vllm
    command:
    - --host
    - 0.0.0.0
    - --port
    - ${VLLM_PORT:-8888}
    - --model
    - ${VLLM_MODEL}
    - --max-model-len
    - ${VLLM_MAX_MODEL_LEN:-8192}
    - --gpu-memory-utilization
    - ${VLLM_GPU_MEMORY:-0.9}
    ports:
    - ${VLLM_PORT:-8888}:8888
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]

  nginx:
    image: nginx:1.27-alpine
    restart: unless-stopped
    depends_on:
    - api
    volumes:
    - ./docker/nginx/conf.d:/etc/nginx/conf.d:ro
    - ./cert.pem:/etc/nginx/certs/cert.pem:ro
    - ./key.pem:/etc/nginx/certs/key.pem:ro
    ports:
    - 80:80
    - 443:443
