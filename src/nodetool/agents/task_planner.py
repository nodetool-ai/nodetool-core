import asyncio
import traceback
from nodetool.chat.providers import ChatProvider
from nodetool.agents.sub_task_context import (
    FILE_POINTER_SCHEMA,
    is_binary_output_type,
    json_schema_for_output_type,
)
from nodetool.agents.tools.base import Tool
from nodetool.metadata.types import (
    Message,
    SubTask,
    Task,
    TaskPlan,
    ToolCall,
)

import json

# Add jsonschema import for validation
import jsonschema
from jsonschema import validate, ValidationError
import yaml
import os
from pathlib import Path
from typing import Any, List, Sequence, Dict, Set, Optional  # Add Optional

from nodetool.workflows.processing_context import ProcessingContext
import time
import networkx as nx

# Add rich imports
from rich.console import Console
from rich.table import Table
from rich.text import Text  # Import Text for better formatting
from rich.live import Live  # Add this import at the top with other Rich imports

# Add Jinja2 imports
from jinja2 import Environment, BaseLoader


# Simplified and phase-agnostic system prompt
DEFAULT_PLANNING_SYSTEM_PROMPT = """
You are TaskArchitect, an expert workflow planning system that converts complex objectives into executable task plans through a multi-phase process (Retrieval, Analysis, Data Flow, Plan Creation).

KEY RESPONSIBILITIES:
1. **Retrieval (Phase 0):** Use provided retrieval tools (if any) to gather necessary information (e.g., URLs, documents) for planning.
2. **Analysis (Phase 1):** Decompose complex tasks into logical, self-contained subtasks based on the objective and retrieved information. Each subtask will be executed by an agent.
3. **Data Flow (Phase 2):** Define precise data dependencies between subtasks (`input_files`, `output_file`, `artifacts`).
4. **Plan Creation (Phase 3):** Generate the final plan using the `create_task` tool, specifying subtask instructions (`content`), inputs, outputs, and schemas.
5. Identify parallel execution opportunities. Instead of running all tools in sequence, run some in parallel if they don't depend on each other's results.
6. For each subtask, provide high-level natural language instructions in the `content` field for the agent executor. The agent will decide how to achieve the objective, potentially using available tools.
7. Ensure type safety (`output_type`, `output_schema`) and relative paths.
8. Optimize for minimal context length.

SUBTASK DESIGN PRINCIPLES (Phases 1-3):
- Each subtask must be atomic and independently executable by an agent.
- **Agent Task Design:**
    - The `content` field MUST contain the high-level *objective* or *instructions* for the subtask executor (agent) in natural language. The agent will then decide how to achieve this, potentially using available tools.
    - Example: `content: "Analyze the sentiment of the text in input.txt and summarize the key findings."`
- Input/output contracts must be explicit and type-safe (`output_type`, `output_schema`).
- **Dependencies MUST form a Directed Acyclic Graph (DAG).** No circular dependencies are allowed.
- File paths (`output_file`, `artifacts`, `input_files`) MUST be relative to the workspace root (e.g., `data/output.csv`). Do NOT include absolute paths or the `workspace/` prefix.
- **Unique File Paths:** Each `output_file` and each file listed in `artifacts` MUST have a unique path within the entire plan to prevent conflicts.
- Subtasks conclude by calling `finish_subtask` with EITHER the result content OR a file pointer `{"path": "relative/path/to/output.ext"}`. The path must be relative.
- **Agent Task `content` Field:** The `content` field MUST contain only high-level instructions. It should NOT contain code, implementation details, commands, or file contents. The executing agent will handle these.
- `artifacts` are additional files generated by the subtask besides `output_file`.
- **Context Optimization:** Design subtasks to be focused to minimize context.
- **Parallel Processing:** Create distinct, parallel subtasks for processing multiple files or items independently. Each should have its own output file. For item processing (e.g., URLs), the `content` describes the action per item (e.g., "Fetch content for the given URL"), and the executor receives the specific item.
"""


def clean_and_validate_path(workspace_dir: str, path: Any, context: str) -> str:
    """
    Cleans workspace prefix and validates a path is relative and safe.
    """
    # Stricter initial type check
    if not isinstance(path, str):
        raise ValueError(
            f"Path must be a string, but got type {type(path)} ('{path}') in {context}."
        )
    if not path.strip():
        raise ValueError(f"Path must be a non-empty string in {context}.")

    # Ensure the path is not absolute
    if Path(path).is_absolute():  # Use pathlib for robustness
        raise ValueError(
            f"Path must be relative, but got absolute path '{path}' in {context}."
        )

    cleaned_path: str = path

    # Remove leading workspace prefixes more robustly
    # Convert potential backslashes for consistency before prefix check
    normalized_prefix_path = cleaned_path.replace("\\", "/")
    if normalized_prefix_path.startswith("workspace/"):
        cleaned_path = cleaned_path[len("workspace/") :]
    # Check for absolute /workspace/ only if it wasn't caught by is_absolute earlier (edge case)
    elif normalized_prefix_path.startswith("/workspace/"):
        cleaned_path = cleaned_path[len("/workspace/") :]

    # After cleaning, double-check it didn't somehow become absolute or empty
    if not cleaned_path:
        raise ValueError(
            f"Path became empty after cleaning prefix: '{path}' in {context}."
        )
    # Re-check absoluteness after potential prefix stripping
    if Path(cleaned_path).is_absolute():
        raise ValueError(
            f"Path became absolute after cleaning prefix: '{path}' -> '{cleaned_path}' in {context}."
        )

    # --- Path Traversal Check ---
    try:
        workspace_root = Path(workspace_dir).resolve(
            strict=True
        )  # Ensure workspace exists
        # Create the full path by joining workspace root and the relative path
        full_path = (workspace_root / cleaned_path).resolve()

        # Check if the resolved path is within the workspace directory
        # Using Path.is_relative_to (Python 3.9+) or common path check
        is_within = False
        try:
            # Preferred method if available
            is_within = full_path.is_relative_to(workspace_root)
        except AttributeError:  # Fallback for Python < 3.9
            is_within = (
                workspace_root == full_path or workspace_root in full_path.parents
            )

        if not is_within:
            raise ValueError(
                f"Path validation failed: Resolved path '{full_path}' is outside the workspace root '{workspace_root}' for input '{path}' in {context}."
            )

    except FileNotFoundError:
        # This occurs if self.workspace_dir doesn't exist during validation
        raise ValueError(
            f"Workspace directory '{workspace_dir}' not found during path validation for '{path}' in {context}."
        )
    except Exception as e:  # Catch other potential resolution or validation errors
        raise ValueError(f"Error validating path safety for '{path}' in {context}: {e}")

    # Return the cleaned, validated relative path
    return cleaned_path


class CreateTaskTool(Tool):
    """
    Task Creator - Tool for generating a task with subtasks
    """

    name = "create_task"
    description = "Create a single task with subtasks"

    input_schema = {
        "type": "object",
        "required": ["title", "subtasks"],
        "additionalProperties": False,
        "properties": {
            "title": {
                "type": "string",
                "description": "The objective of the task",
            },
            "subtasks": {
                "type": "array",
                "description": "The subtasks of the task",
                "items": {
                    "type": "object",
                    "additionalProperties": False,
                    "properties": {
                        "content": {
                            "type": "string",
                            "description": "High-level natural language instructions for the agent executing this subtask.",
                        },
                        "output_file": {
                            "type": "string",
                            "description": "The file path where the subtask will save its output. MUST be relative to the workspace root (e.g., 'results/data.json'). Do NOT use absolute paths or '/workspace/' prefix.",
                        },
                        "artifacts": {
                            "type": "array",
                            "items": {
                                "type": "string",
                                "description": "An artifact file generated by the subtask. MUST be a relative path (e.g., 'figures/plot.png'). These can be used as inputs for other subtasks.",
                            },
                        },
                        "input_files": {
                            "type": "array",
                            "items": {
                                "type": "string",
                                "description": "An input file for the subtask. MUST be a relative path (e.g., 'inputs/source.txt') corresponding to an initial input file, the output_file of another subtask, or an artifact from another subtask.",
                            },
                        },
                        "output_schema": {
                            "type": "string",
                            "description": 'Output schema for the subtask as a JSON string. Use \'{"type": "string"}\' for unstructured output types.',
                        },
                        "output_type": {
                            "type": "string",
                            "description": "The file format of the output of the subtask, e.g. 'json', 'markdown', 'csv', 'html'",
                        },
                    },
                    "required": [
                        "content",
                        "output_file",
                        "output_type",
                        "output_schema",
                        "input_files",
                        # tool_name removed from required
                        "artifacts",
                    ],
                },
            },
        },
    }

    async def process(self, context: ProcessingContext, params: dict):
        # Validation is handled by the TaskPlanner when processing the LLM's response
        # using this tool schema, specifically in _process_subtask_list.
        pass  # No validation needed here


# Make sure the phase prompts are concrete and focused
RETRIEVAL_PHASE_TEMPLATE = """
# PHASE 0: RETRIEVAL

Gather essential information needed to effectively plan for the objective using the available retrieval tools. Analyze the objective and input files to determine what information is missing or required.

- Use the retrieval tools provided below to find relevant data, URLs, documentation, or examples.
- Focus solely on gathering information. Do NOT attempt to create the task plan or define subtasks yet.
- The results of your retrieval will inform the subsequent planning phases.

User's Objective: {{ objective }}

Retrieval Tools Available:
{{ retrieval_tools_info }}

Input Files:
{{ input_files_info }}

Perform necessary retrieval actions using the available tools.
"""

ANALYSIS_PHASE_TEMPLATE = """
# PHASE 1: ANALYSIS

Analyze the objective, input files, and any information gathered during the Retrieval phase (available in the history) to create a strategic execution plan.

- Break down the objective into distinct, atomic subtasks. Each subtask will be executed by an agent using the instructions you provide.
- Identify opportunities for parallel execution.
- Consider file paths (relative), dependencies, and result passing (content vs. file pointer).
- Plan for separate subtasks for each input file or item if processing independently.
- Design for minimal execution context.

User's Objective: {{ objective }}

Execution Tools Available (for the Agent to potentially use):
{{ execution_tools_info }}

Input Files:
{{ input_files_info }}

**IMPORTANT:** Output a clear, conceptual list (e.g., numbered or bulleted) of the subtasks you have identified based on your analysis. This list will be used in subsequent phases. Define the *instructions* for each subtask agent.

DO NOT USE TOOL CALLS DURING THIS PHASE. Focus on the conceptual breakdown and listing the subtasks with their instructions.
"""

DATA_FLOW_ANALYSIS_TEMPLATE = """
# PHASE 2: DATA FLOW ANALYSIS

Refine the plan, focusing on data flow and contracts, using the conceptual subtask list identified in Phase 1 (Analysis), which is available in the conversation history.

- Map information flow between the identified subtasks (all executed by agents).
- Define inputs (`input_files`) and outputs (`output_file`, `artifacts`) for each subtask from the Analysis phase.
- Specify `output_type` and `output_schema` for each subtask.
- Plan how results are passed (`finish_subtask` with content or file pointer `{"path": "..."}`).
- Ensure dependencies are clear and acyclic.
- Plan relative file paths and naming conventions.
- **Instruction Refinement:** Refine the natural language `content` (instructions) for each agent subtask based on the conceptual list from Phase 1.
- **Consider the overall task output requirements:** The final subtask(s) should produce output matching the overall task's expected output type: `{{ overall_output_type }}` and schema: `{{ overall_output_schema }}`.

User's Objective: {{ objective }}

Overall Task Output Type: {{ overall_output_type }}
Overall Task Output Schema: {{ overall_output_schema }}

Conceptual Subtasks from Phase 1 (Refer to history): [LLM should recall the list from Phase 1]

Execution Tools Available (for the Agent to potentially use):
{{ execution_tools_info }}

Input Files:
{{ input_files_info }}

DO NOT USE TOOL CALLS DURING THIS PHASE. Finalize the data contracts and instructions for the subtasks identified previously, ensuring the final output aligns with the overall task requirements.
"""

PLAN_CREATION_TEMPLATE = """
# PHASE 3: PLAN CREATION

Transform your analysis and data flow design into a concrete, executable task plan using the `create_task` tool. Implement the subtasks identified in Phase 1 (Analysis) and refined in Phase 2 (Data Flow), using the information available in the conversation history.

## Implementation Requirements
1. Create subtasks implementing the components identified and refined in the previous phases.
2. Define precise relative file paths (`input_files`, `output_file`, `artifacts`). No absolute paths or `/workspace/` prefix.
3. **Unique File Paths:** Ensure *all* generated file paths (`output_file` and all paths within `artifacts` across all subtasks) are unique to avoid overwriting files.
4. **Subtask Instructions:** The `content` field MUST contain high-level natural language instructions for the agent executing the subtask. Do NOT include `tool_name`.
5. Specify correct `output_type` and `output_schema` (as a JSON string) for each subtask.
6. Ensure dependencies are correctly expressed via `input_files`, reflecting the data flow defined in Phase 2.
7. Maximize parallel execution based on the dependency analysis.
8. **Minimize Context:** Ensure subtasks remain focused, as designed earlier.
9. **Agent Task `content`:** High-level objective only. No code, implementation details, or large data.
10. **Final Output:** The overall plan MUST produce a final result that conforms to the overall task output type `{{ overall_output_type }}` and schema `{{ overall_output_schema }}`. Ensure the final subtask(s) are designed accordingly.

## Finishing Subtasks
- Each subtask calls `finish_subtask`.
- Pass result content (matching `output_schema`) or a file pointer `{"path": "relative/path/..."}`.

## Validation Criteria
- No circular dependencies.
- All inputs available when needed.
- `content` is a natural language instruction for the agent.
- Final output matches overall task requirements (Type: `{{ overall_output_type }}`, Schema: `{{ overall_output_schema }}`).
- Schemas (`output_schema`) are correctly defined JSON strings.
- File paths are relative.

User's Objective: {{ objective }}

Overall Task Output Type: {{ overall_output_type }}
Overall Task Output Schema: {{ overall_output_schema }}

Conceptual Subtasks & Data Flow (Refer to history): [LLM should recall details from Phase 1 & 2]

Execution Tools Available (for the Agent to potentially use):
{{ execution_tools_info }}

Input Files:
{{ input_files_info }}

CREATE YOUR PLAN NOW using the `create_task` tool. Ensure your plan implements the subtasks and data flow established in the previous phases. Ensure `content` contains clear agent instructions and the final output meets the specified overall requirements.
"""

# Agent task prompt used in the final planning stage
DEFAULT_AGENT_TASK_TEMPLATE = """
# TASK PLANNING OBJECTIVE

Create an optimal execution plan for: {{ objective }}

## Available Resources
Execution Tools:
{{ execution_tools_info }}
Retrieval Tools (already used in Phase 0):
{{ retrieval_tools_info }}


## Input Files
{{ input_files_info }}

## Previously Gathered Information
(Available in the conversation history from Phase 0: Retrieval)

Design a plan where each subtask has:
1. **Task Type Defined:**
    - **Tool Task:** `tool_name` is set (from Execution Tools), `content` is JSON arguments for the tool.
    - **Agent Task:** `tool_name` is empty, `content` is high-level instructions for the agent.
2. Well-defined relative paths (`input_files`, `output_file`, `artifacts`). No absolute paths or `/workspace/` prefix.
3. Appropriate `output_type` and `output_schema` (JSON string).
4. Minimal dependencies.
5. Clear understanding of how `finish_subtask` is used (content vs. file pointer).
6. **Minimal Context Focus:** Small, focused subtasks. One subtask per file/item for parallel processing.
"""


class TaskPlanner:
    """
    Orchestrates the breakdown of a complex objective into a validated, executable
    workflow plan (`TaskPlan`) composed of interdependent subtasks (`SubTask`).

    Think of this as the lead architect for an AI agent system. It doesn't execute
    the subtasks itself, but meticulously designs the blueprint. Given a high-level
    objective (e.g., "Analyze market trends for product X"), the TaskPlanner uses
    an LLM to generate a structured plan detailing:

    1.  **Decomposition:** Breaking the objective into smaller, logical, and ideally
        atomic units of work (subtasks).
    2.  **Task Typing:** Determining if each subtask is a straightforward,
        deterministic call to a specific `Tool` (e.g., download a file) or if it
        requires more complex reasoning or multiple steps better handled by a
        probabilistic `Agent` executor (e.g., summarize analysis findings).
    3.  **Data Flow & Dependencies:** Explicitly defining the inputs (`input_files`)
        and outputs (`output_file`, `artifacts`) for each subtask. Crucially, it
        establishes the dependency graph, ensuring subtasks run only after their
        required inputs are available. This forms a Directed Acyclic Graph (DAG).
    4.  **Contracts:** Defining the expected data format (`output_type`,
        `output_schema`) for each subtask's output, promoting type safety and
        predictable integration between steps.
    5.  **Workspace Management:** Enforcing the use of *relative* file paths within
        a defined workspace, preventing dangerous absolute path manipulations and
        ensuring plan portability. Paths like `/tmp/foo` or `C:\\Users\\...` are
        strictly forbidden; only paths like `data/interim_results.csv` are valid.
    6.  **Optimization:** Identifying opportunities for parallel execution where
        subtasks have no dependencies on each other.

    The planning process itself can involve multiple phases (configurable):
    - **Retrieval Phase (Optional):** Uses dedicated retrieval tools to gather
      information (e.g., search results, document contents) needed for planning.
    - **Analysis Phase:** High-level strategic breakdown and identification of
      subtask types (Tool vs. Agent).
    - **Data Flow Analysis:** Refining dependencies, inputs/outputs, and data schemas.
    - **Plan Creation:** Generating the final, concrete `Task` object using the LLM,
      typically by invoking an internal `CreateTaskTool` or leveraging structured
      output capabilities if supported by the LLM provider.

    **Core Responsibility:** To transform an ambiguous user objective into an unambiguous,
    validated, and machine-executable plan. It prioritizes structure, clear contracts,
    and dependency management over monolithic, error-prone LLM interactions. This
    structured approach is essential for reliable, efficient, and debuggable AI workflows.

    **Validation is paramount:** The planner rigorously checks the generated plan for:
    - Cyclic dependencies (fatal).
    - Missing input files.
    - Correct `tool_name` usage and valid JSON arguments for Tool tasks.
    - Correct `content` format (natural language instructions) for Agent tasks.
    - Valid and relative file paths.
    - Schema consistency.

    A plan that fails validation is rejected, forcing the LLM to correct its mistakes.
    This avoids garbage-in, garbage-out execution downstream.

    Attributes:
        provider (ChatProvider): The LLM provider instance used for generation.
        model (str): The specific LLM model identifier.
        objective (str): The high-level goal the plan aims to achieve.
        workspace_dir (str): The root directory for all relative file paths.
        input_files (List[str]): Initial files available at the start of the plan.
        retrieval_tools (Sequence[Tool]): Tools available *only* during the Retrieval phase.
        execution_tools (Sequence[Tool]): Tools available for subtasks designated as Tool tasks
                                         during the Plan Creation phase.
        task_plan (Optional[TaskPlan]): The generated plan (populated after creation).
        system_prompt (str): The core instructions guiding the LLM planner.
        output_schema (Optional[dict]): Optional schema for the *final* output of the
                                        overall task (not individual subtasks).
        enable_retrieval_phase (bool): Controls whether the initial retrieval phase runs.
        enable_analysis_phase (bool): Controls whether the analysis phase runs.
        enable_data_contracts_phase (bool): Controls whether the data contract phase runs.
        use_structured_output (bool): If True, attempts to use LLM's structured output
                                      features for plan generation, otherwise uses tool calls.
        verbose (bool): Enables detailed logging and progress display during planning.
        console (Optional[Console]): Rich console instance for verbose output.
        live (Optional[Live]): Rich live display handler for planning phases.
        jinja_env (Environment): Jinja2 environment for rendering prompts.
        tasks_file_path (Path): Path where the plan might be saved/loaded (`tasks.yaml`).
    """

    def __init__(
        self,
        provider: ChatProvider,
        model: str,
        objective: str,
        workspace_dir: str,
        execution_tools: Sequence[Tool],
        retrieval_tools: Sequence[Tool] = [],
        input_files: Sequence[str] = [],
        system_prompt: str | None = None,
        output_schema: dict | None = None,
        output_type: str | None = None,
        enable_retrieval_phase: bool = True,
        enable_analysis_phase: bool = True,
        enable_data_contracts_phase: bool = True,
        use_structured_output: bool = False,
        verbose: bool = True,
    ):
        """
        Initialize the TaskPlanner.

        Args:
            provider (ChatProvider): An LLM provider instance
            model (str): The model to use with the provider
            objective (str): The objective to solve
            workspace_dir (str): The workspace directory path
            execution_tools (List[Tool]): Tools available for subtask execution.
            retrieval_tools (List[Tool], optional): Tools available only for the retrieval phase.
            input_files (list[str]): The input files to use for planning
            system_prompt (str, optional): Custom system prompt
            output_schema (dict, optional): JSON schema for the final task output
            output_type (str, optional): The type of the final task output
            enable_retrieval_phase (bool, optional): Whether to run the retrieval phase (PHASE 0)
            enable_analysis_phase (bool, optional): Whether to run the analysis phase (PHASE 1)
            enable_data_contracts_phase (bool, optional): Whether to run the data contracts phase (PHASE 2)
            use_structured_output (bool, optional): Whether to use structured output for plan creation
            verbose (bool, optional): Whether to print planning progress table (default: True)
        """
        self.provider: ChatProvider = provider
        self.model: str = model
        self.objective: str = objective
        self.workspace_dir: str = workspace_dir
        self.task_plan: Optional[TaskPlan] = None
        # Clean and validate initial input files relative to workspace
        self.input_files: List[str] = [
            self._clean_and_validate_path(f, "initial input files") for f in input_files
        ]
        self.system_prompt: str = self._customize_system_prompt(system_prompt)

        self.retrieval_tools: Sequence[Tool] = retrieval_tools or []
        self.execution_tools: Sequence[Tool] = execution_tools or []
        self.output_schema: Optional[dict] = output_schema
        self.output_type: Optional[str] = output_type
        self.enable_retrieval_phase: bool = enable_retrieval_phase
        self.enable_analysis_phase: bool = enable_analysis_phase
        self.enable_data_contracts_phase: bool = enable_data_contracts_phase
        self.use_structured_output: bool = use_structured_output
        self.verbose: bool = verbose

        self.tasks_file_path: Path = Path(workspace_dir) / "tasks.yaml"
        self.console: Optional[Console] = Console() if self.verbose else None
        self.live: Optional[Live] = (
            None
            if not self.verbose
            else Live(
                Table(
                    title="Task Planning Phases",
                    show_header=True,
                    header_style="bold magenta",
                    show_lines=True,
                ),
                console=self.console,
                refresh_per_second=4,
                vertical_overflow="visible",
            )
        )
        # Initialize Jinja2 environment
        self.jinja_env: Environment = Environment(loader=BaseLoader())

    def _clean_and_validate_path(self, path: Any, context: str) -> str:
        """
        Clean and validate a path relative to the workspace directory.
        """
        return clean_and_validate_path(self.workspace_dir, path, context)

    def _customize_system_prompt(self, system_prompt: str | None) -> str:
        """
        Customize the system prompt based on provider capabilities.

        Args:
            system_prompt: Optional custom system prompt

        Returns:
            str: The customized system prompt
        """
        if system_prompt:
            base_prompt = system_prompt
        else:
            base_prompt = DEFAULT_PLANNING_SYSTEM_PROMPT

        return str(base_prompt)

    async def _load_existing_plan(self) -> bool:
        """
        Try to load an existing task plan from the workspace.

        Returns:
            bool: True if plan was loaded successfully, False otherwise
        """
        if self.tasks_file_path.exists():
            try:
                with open(self.tasks_file_path, "r") as f:
                    task_plan_data: dict = yaml.safe_load(f)
                    self.task_plan = TaskPlan(**task_plan_data)
                    return True
            except Exception:  # Keep general exception for file I/O or parsing issues
                return False
        return False

    def _get_prompt_context(self) -> Dict[str, str]:
        """Helper to build the context for Jinja2 rendering."""
        # Provide default string representation if schema/type are None or not set
        overall_output_schema_str = (
            json.dumps(self.output_schema)
            if self.output_schema
            else "Not specified (default: string)"
        )
        overall_output_type_str = (
            self.output_type if self.output_type else "Not specified (default: string)"
        )

        return {
            "objective": self.objective,
            "retrieval_tools_info": self._get_retrieval_tools_info(),
            "execution_tools_info": self._get_execution_tools_info(),
            "input_files_info": self._get_input_files_info(),
            "overall_output_type": overall_output_type_str,
            "overall_output_schema": overall_output_schema_str,
        }

    def _render_prompt(
        self, template_string: str, context: Optional[Dict[str, str]] = None
    ) -> str:
        """Renders a prompt template using Jinja2."""
        if context is None:
            context = self._get_prompt_context()
        template = self.jinja_env.from_string(template_string)
        return template.render(context)

    async def _build_agent_task_prompt_content(self) -> str:
        """Builds the content for the agent task prompt using Jinja2."""
        return self._render_prompt(DEFAULT_AGENT_TASK_TEMPLATE)

    def _build_dependency_graph(self, subtasks: List[SubTask]) -> nx.DiGraph:
        """
        Build a directed graph of dependencies between subtasks.

        Args:
            subtasks: List of subtasks to analyze

        Returns:
            nx.DiGraph: Directed graph representing dependencies
        """
        # Create mapping of output files AND artifacts to their subtasks
        output_to_subtask: Dict[str, SubTask] = {}
        for subtask in subtasks:
            output_to_subtask[subtask.output_file] = subtask
            if subtask.artifacts:
                for artifact in subtask.artifacts:
                    # If an artifact path conflicts with an output_file, the output_file takes precedence
                    if artifact not in output_to_subtask:
                        output_to_subtask[artifact] = subtask

        G = nx.DiGraph()

        # Add nodes representing subtasks (using output_file as a unique ID proxy for the node)
        for subtask in subtasks:
            G.add_node(subtask.output_file)  # Node represents the subtask completion

        # Add edges for dependencies: from the *output file* of the dependency task to the *output file* of the current task
        for subtask in subtasks:
            if subtask.input_files:
                for input_file in subtask.input_files:
                    if input_file in output_to_subtask:
                        # Get the subtask that produces this input file
                        producer_subtask = output_to_subtask[input_file]
                        # Add edge from the producer's node to the current subtask's node
                        G.add_edge(producer_subtask.output_file, subtask.output_file)

        return G

    def _check_output_file_conflicts(
        self, subtasks: List[SubTask]
    ) -> tuple[List[str], Set[str]]:
        """Checks for duplicate output files and artifacts."""
        validation_errors: List[str] = []
        output_files: Dict[str, SubTask] = {}
        all_generated_files: Set[str] = set()

        for i, subtask in enumerate(subtasks):
            sub_context = f"Subtask {i} ('{subtask.content[:30]}...')"
            # Check primary output file
            if subtask.output_file in output_files:
                validation_errors.append(
                    f"{sub_context}: Multiple subtasks trying to write primary output to '{subtask.output_file}'"
                )
            elif subtask.output_file in all_generated_files:
                validation_errors.append(
                    f"{sub_context}: Primary output file '{subtask.output_file}' conflicts with an artifact from another task."
                )
            else:
                output_files[subtask.output_file] = subtask
                all_generated_files.add(subtask.output_file)

            # Check artifacts for conflicts
            if subtask.artifacts:
                for artifact in subtask.artifacts:
                    if artifact in output_files:
                        validation_errors.append(
                            f"{sub_context}: Artifact '{artifact}' conflicts with the primary output file of another task."
                        )
                    elif artifact in all_generated_files:
                        validation_errors.append(
                            f"{sub_context}: Duplicate artifact path '{artifact}' detected across subtasks."
                        )
                    all_generated_files.add(artifact)
        return validation_errors, all_generated_files

    def _check_input_file_availability(
        self, subtasks: List[SubTask], all_generated_files: Set[str]
    ) -> List[str]:
        """Checks if all input files for subtasks are available."""
        validation_errors: List[str] = []
        available_files: Set[str] = set(self.input_files)

        for subtask in subtasks:
            if subtask.input_files:
                for file_path in subtask.input_files:
                    if (
                        file_path not in available_files
                        and file_path not in all_generated_files
                    ):
                        validation_errors.append(
                            f"Subtask '{subtask.content}' depends on missing file '{file_path}'"
                        )
        return validation_errors

    def _validate_dependencies(self, subtasks: List[SubTask]) -> List[str]:
        """
        Validate dependencies, file conflicts, and DAG structure for subtasks.
        """
        validation_errors: List[str] = []

        # 1. Check for output file conflicts
        output_conflict_errors, all_generated_files = self._check_output_file_conflicts(
            subtasks
        )
        validation_errors.extend(output_conflict_errors)

        # 2. Build dependency graph
        G = self._build_dependency_graph(subtasks)

        # 3. Check for cycles
        try:
            cycle = nx.find_cycle(G)
            validation_errors.append(f"Circular dependency detected: {cycle}")
        except nx.NetworkXNoCycle:
            pass  # No cycles found, which is good

        # 4. Validate all input files exist before their use
        input_availability_errors = self._check_input_file_availability(
            subtasks, all_generated_files
        )
        validation_errors.extend(input_availability_errors)

        # 5. Check if a valid execution order exists (topological sort)
        if not validation_errors:  # Only check if no other critical errors found
            try:
                list(nx.topological_sort(G))
            except nx.NetworkXUnfeasible:
                # This might be redundant if cycle check or input availability check failed,
                # but provides an extra layer of verification.
                validation_errors.append(
                    "Cannot determine valid execution order due to unresolved dependency issues (potentially complex cycle or missing input)."
                )

        return validation_errors

    def _update_planning_table(
        self,
        table: Optional[Table],
        phase_name: str,
        status: str,
        content: str | Text,
        is_error: bool = False,
    ) -> None:
        """Helper to update the planning progress table."""
        if table and self.live and self.live.is_started:
            status_style = (
                "bold red"
                if is_error
                else "bold green" if status == "Success" else "bold yellow"
            )
            # Truncate long content for better table display
            content_str = str(content)
            if len(content_str) > 1000:
                content_str = content_str[:1000] + "..."
            status_text = Text(status, style=status_style)
            # Use Text object if already Text, otherwise create one
            content_text = content if isinstance(content, Text) else Text(content_str)
            table.add_row(phase_name, status_text, content_text)
            self.live.update(table)  # Update the live display

    async def _process_retrieval_tool_call(
        self,
        tc: ToolCall,
        context: ProcessingContext,
        history: List[Message],
        table: Optional[Table],
        round_num: int,
    ) -> bool:
        """Processes a single tool call during the retrieval phase."""
        tool_result: Optional[dict] = None
        tool_found: bool = False
        error_content: Optional[str] = None

        for tool in self.retrieval_tools:
            if tool.name == tc.name:
                tool_found = True
                try:
                    tool_result = await tool.process(context, tc.args)
                except Exception as tool_err:
                    error_content = f"Error processing tool {tool.name}: {tool_err}"
                    self._update_planning_table(
                        table,
                        f"0. Retrieval (Round {round_num}, Tool {tool.name})",
                        "Tool Error",
                        Text(error_content, style="bold red"),
                        is_error=True,
                    )
                break

        if not tool_found:
            error_content = f"Retrieval tool '{tc.name}' requested by LLM not found."
            self._update_planning_table(
                table,
                f"0. Retrieval (Round {round_num}, Tool {tc.name})",
                "Tool Error",
                Text(error_content, style="bold red"),
                is_error=True,
            )

        # Add tool response message to history
        response_content_obj: dict
        if error_content:
            response_content_obj = {"error": error_content}
        else:
            response_content_obj = tool_result if tool_result is not None else {}

        history.append(
            Message(
                role="tool",
                tool_call_id=tc.id,
                content=json.dumps(response_content_obj),
            )
        )
        return True  # Indicates a tool response was added

    async def _run_retrieval_phase(
        self,
        history: List[Message],
        context: ProcessingContext,
        table: Optional[Table],
        max_retrieval_rounds: int,
    ) -> tuple[List[Message], Optional[Message]]:
        """Handles Phase 0: Retrieval."""
        if not self.enable_retrieval_phase:
            self._update_planning_table(
                table, "0. Retrieval", "Skipped", "Phase disabled."
            )
            return history, None
        if not self.retrieval_tools:
            self._update_planning_table(
                table, "0. Retrieval", "Skipped", "No retrieval tools provided."
            )
            return history, None

        retrieval_prompt_content = self._render_prompt(RETRIEVAL_PHASE_TEMPLATE)
        history.append(Message(role="user", content=retrieval_prompt_content))
        self._update_planning_table(
            table, "0. Retrieval", "Running", "Starting multi-round retrieval..."
        )

        retrieval_rounds = 0
        last_retrieval_response: Optional[Message] = None
        while retrieval_rounds < max_retrieval_rounds:
            retrieval_rounds += 1
            retrieval_response = await self.provider.generate_message(
                messages=history, model=self.model, tools=self.retrieval_tools
            )
            history.append(retrieval_response)
            last_retrieval_response = retrieval_response

            if not retrieval_response.tool_calls:
                self._update_planning_table(
                    table,
                    f"0. Retrieval (Round {retrieval_rounds})",
                    "Completed",
                    f"LLM finished retrieval. Final message:\n{self._format_message_content(retrieval_response)}",
                )
                break  # Exit loop if no tool calls

            tool_results_added_this_round = False
            for tc in retrieval_response.tool_calls:
                # Process each tool call using the helper
                if await self._process_retrieval_tool_call(
                    tc, context, history, table, retrieval_rounds
                ):
                    tool_results_added_this_round = True

            if tool_results_added_this_round:
                self._update_planning_table(
                    table,
                    f"0. Retrieval (Round {retrieval_rounds})",
                    "Tool(s) Called",
                    self._format_message_content(retrieval_response),
                )

        # --- After loop ---
        if (
            retrieval_rounds >= max_retrieval_rounds
        ):  # Loop finished due to max_retrieval_rounds
            self._update_planning_table(
                table,
                f"0. Retrieval",
                "Max Rounds Reached",
                (
                    f"Stopped after {max_retrieval_rounds} rounds. Last message:\n{self._format_message_content(last_retrieval_response)}"
                    if last_retrieval_response
                    else f"Stopped after {max_retrieval_rounds} rounds."
                ),
                is_error=True,
            )

        # Debugging print statements (optional, consider removing for production)
        # print("==" * 100)
        # for m in history:
        #     print(json.dumps(m.model_dump(exclude_none=True), indent=2))
        # print("==" * 100)

        return history, last_retrieval_response

    async def _run_analysis_phase(
        self, history: List[Message], table: Optional[Table]
    ) -> tuple[List[Message], Optional[Message]]:
        """Handles Phase 1: Analysis."""
        if not self.enable_analysis_phase:
            self._update_planning_table(
                table, "1. Analysis", "Skipped", "Phase disabled."
            )
            return history, None

        analysis_prompt_content: str = self._render_prompt(ANALYSIS_PHASE_TEMPLATE)
        history.append(Message(role="user", content=analysis_prompt_content))

        analysis_message: Message = await self.provider.generate_message(
            messages=history, model=self.model, tools=[]  # Explicitly empty list
        )
        history.append(analysis_message)

        phase_status: str = "Completed"
        phase_content: str | Text = self._format_message_content(analysis_message)
        self._update_planning_table(table, "1. Analysis", phase_status, phase_content)

        return history, analysis_message

    async def _run_data_flow_phase(
        self, history: List[Message], table: Optional[Table]
    ) -> tuple[List[Message], Optional[Message]]:
        """Handles Phase 2: Data Flow Analysis."""
        if not self.enable_data_contracts_phase:
            self._update_planning_table(
                table, "2. Data Contracts", "Skipped", "Phase disabled."
            )
            return history, None

        data_flow_prompt_content: str = self._render_prompt(DATA_FLOW_ANALYSIS_TEMPLATE)
        history.append(Message(role="user", content=data_flow_prompt_content))

        data_contracts_message: Message = await self.provider.generate_message(
            messages=history, model=self.model, tools=[]  # Explicitly empty list
        )
        history.append(data_contracts_message)

        phase_status: str = "Completed"
        phase_content: str | Text = self._format_message_content(data_contracts_message)
        self._update_planning_table(
            table, "2. Data Contracts", phase_status, phase_content
        )

        return history, data_contracts_message

    async def _run_plan_creation_phase(
        self,
        history: List[Message],
        objective: str,
        table: Optional[Table],
        max_retries: int,
    ) -> tuple[Optional[Task], Optional[Message], Optional[Exception]]:
        """Handles Phase 3: Plan Creation."""
        task: Optional[Task] = None
        final_message: Optional[Message] = None
        plan_creation_error: Optional[Exception] = None
        phase_status: str = "Failed"
        phase_content: str | Text = "N/A"
        current_phase_name: str = "3. Plan Creation"

        if self.use_structured_output:
            self._update_planning_table(
                table,
                current_phase_name,
                "Running",
                "Attempting plan creation using structured output...",
            )
            try:
                task = await self._create_task_with_structured_output(
                    objective, max_retries
                )
                phase_status = "Success"
                phase_content = f"Plan created with {len(task.subtasks)} subtasks using structured output."
                final_message = None  # Not applicable for structured output path
            except Exception as e:
                plan_creation_error = e
                phase_status = "Failed"
                phase_content = Text(
                    f"Structured output failed: {str(e)}\n{traceback.format_exc()}",
                    style="bold red",
                )
        else:  # Use tool-based generation
            plan_creation_prompt_content = self._render_prompt(PLAN_CREATION_TEMPLATE)
            agent_task_prompt_content = await self._build_agent_task_prompt_content()
            history.append(
                Message(
                    role="user",
                    content=f"{plan_creation_prompt_content}\n{agent_task_prompt_content}",
                )
            )
            self._update_planning_table(
                table,
                current_phase_name,
                "Running",
                "Attempting plan creation using the 'create_task' tool...",
            )
            try:
                task, final_message = await self._generate_with_retry(
                    history,
                    tools=[CreateTaskTool()],
                    max_retries=max_retries,
                )

                if task:
                    phase_status = "Success"
                    phase_content = (
                        self._format_message_content(final_message)
                        if final_message
                        else "Plan created using tool calls."
                    )
                else:
                    failure_reason = "Unknown failure after retries."
                    if final_message and (
                        final_message.content or final_message.tool_calls
                    ):
                        formatted_content = self._format_message_content(final_message)
                        if (
                            "error" in str(formatted_content).lower()
                            or "fail" in str(formatted_content).lower()
                        ):
                            failure_reason = (
                                f"LLM indicated failure: {formatted_content}"
                            )
                        else:
                            failure_reason = f"LLM did not produce a valid 'create_task' tool call. Last message: {formatted_content}"
                    elif (
                        plan_creation_error
                    ):  # Check if _generate_with_retry raised an error internally
                        failure_reason = f"Tool call generation failed internally: {plan_creation_error}"

                    plan_creation_error = ValueError(
                        f"Tool call generation failed: {failure_reason}"
                    )
                    phase_content = Text(
                        f"Tool call generation failed: {failure_reason}",
                        style="bold red",
                    )
                    phase_status = "Failed"
            except Exception as e:
                plan_creation_error = e
                phase_status = "Failed"
                phase_content = Text(
                    f"Tool call generation failed: {str(e)}\n{traceback.format_exc()}",
                    style="bold red",
                )

        # Update Table for Phase 3
        self._update_planning_table(
            table,
            current_phase_name,
            phase_status,
            phase_content,
            is_error=(phase_status == "Failed"),
        )

        return task, final_message, plan_creation_error

    async def _create_task_for_objective(
        self,
        context: ProcessingContext,
        objective: str,
        max_retries: int = 3,
        max_retrieval_rounds: int = 5,
    ) -> Task:
        """
        Create subtasks using the configured planning process, allowing for early shortcuts.
        Displays a live table summarizing the planning process if verbose mode is enabled.
        """
        # --- Rich Table Setup ---
        table: Optional[Table] = None
        if self.console and self.live:
            table = Table(
                "Phase",
                "Status",
                "Reasoning / Output / Errors",
                show_header=True,
            )
            self.live.start()
            self.live.update(table)

        history: List[Message] = [
            Message(role="system", content=self.system_prompt),
        ]

        last_message: Optional[Message] = None
        error_message: Optional[str] = None
        plan_creation_error: Optional[Exception] = None
        task: Optional[Task] = None

        try:
            # Phase 0: Retrieval
            history, last_msg_retrieval = await self._run_retrieval_phase(
                history, context, table, max_retrieval_rounds
            )
            if last_msg_retrieval:
                last_message = last_msg_retrieval

            # Phase 1: Analysis
            history, last_msg_analysis = await self._run_analysis_phase(history, table)
            if last_msg_analysis:
                last_message = last_msg_analysis

            # Phase 2: Data Flow Analysis
            history, last_msg_data_flow = await self._run_data_flow_phase(
                history, table
            )
            if last_msg_data_flow:
                last_message = last_msg_data_flow

            # Phase 3: Plan Creation
            task, last_msg_plan, plan_creation_error = (
                await self._run_plan_creation_phase(
                    history, objective, table, max_retries
                )
            )
            if last_msg_plan:
                last_message = last_msg_plan

            # --- Final Outcome ---
            if task:
                if self.console:
                    self.console.print(
                        "[bold green]Plan created successfully.[/bold green]"
                    )
                return task  # Task is guaranteed to be non-None here
            else:
                # Construct error message based on plan_creation_error or last message
                if plan_creation_error:
                    error_message = f"Failed to create valid task during Plan Creation phase. Original error: {str(plan_creation_error)}"
                    full_error_message = (
                        f"{error_message}\n{traceback.format_exc()}"
                        if self.verbose
                        else error_message
                    )
                    raise ValueError(full_error_message) from plan_creation_error
                else:
                    last_msg_content = (
                        self._format_message_content(last_message)
                        if last_message
                        else "No final message recorded."
                    )
                    error_message = f"Failed to create valid task after maximum retries in Plan Creation phase for an unknown reason. Last message: {last_msg_content}"
                    raise ValueError(error_message)

        except Exception as e:
            # Capture the original exception type and message
            error_message = f"Planning failed: {type(e).__name__}: {str(e)}"

            # Log traceback if verbose or not using live display
            if self.verbose and (not self.live or not self.live.is_started):
                if self.console:
                    self.console.print_exception(show_locals=False)
            elif not self.verbose:
                pass  # Consider logging traceback elsewhere

            # Add error row to table if it's running
            if table and self.live and self.live.is_started and error_message:
                self._update_planning_table(
                    table,
                    "Overall Status",
                    "Failed",
                    Text(
                        f"{error_message}\n{traceback.format_exc()}", style="bold red"
                    ),
                    is_error=True,
                )
            # Print error to console otherwise
            elif self.console:
                self.console.print(
                    f"[bold red]Planning Error:[/bold red] {error_message}"
                )
                # Print traceback separately if console exists but live display wasn't used/stopped
                if not self.live or not self.live.is_started:
                    self.console.print_exception(show_locals=False)

            raise  # Re-raise the caught exception

        finally:
            if self.live and self.live.is_started:
                self.live.stop()
        # This path should not be reachable if an error occurred or task was successful
        raise RuntimeError(
            "Task planning finished unexpectedly without a result or error."
        )

    def _format_message_content(self, message: Optional[Message]) -> str | Text:
        """Formats message content for table display. (Handles None message)"""
        if not message:
            return Text("No response message.", style="dim")
        if message.tool_calls:
            # Summarize tool calls
            calls_summary: List[str] = []
            for tc in message.tool_calls:
                # Truncate args if too long for table display
                args_str = json.dumps(tc.args)
                if len(args_str) > 200:
                    args_str = args_str[:200] + "..."
                calls_summary.append(f"- Tool Call: {tc.name}\n  Args: {args_str}")
            # Use Text object for potential future styling
            return Text("\n".join(calls_summary))
        elif message.content:
            # Return raw content (often reasoning)
            if isinstance(message.content, list):
                # Attempt to join list items; handle potential non-string items
                try:
                    return Text("\n".join(str(item) for item in message.content))
                except Exception:
                    return Text(
                        str(message.content)
                    )  # Fallback to string representation of the list
            elif isinstance(message.content, str):
                return Text(message.content)
            else:
                # Handle other unexpected content types
                return Text(
                    f"Unexpected content type: {type(message.content).__name__}"
                )

        else:
            return Text("Empty message content.", style="dim")

    def _validate_tool_task(
        self,
        subtask_data: dict,
        tool_name: str,
        content: Any,
        available_execution_tools: Dict[str, Tool],
        sub_context: str,
    ) -> tuple[Optional[dict], List[str]]:
        """Validates a subtask intended as a direct tool call."""
        validation_errors: List[str] = []
        parsed_content: Optional[dict] = None

        if tool_name not in available_execution_tools:
            validation_errors.append(
                f"{sub_context}: Specified tool_name '{tool_name}' is not in the list of available execution tools: {list(available_execution_tools.keys())}."
            )
            return None, validation_errors

        tool_to_use = available_execution_tools[tool_name]

        # Validate content is JSON and parse it
        if isinstance(content, dict):
            parsed_content = content  # Already a dict
        elif isinstance(content, str):
            try:
                parsed_content = json.loads(content) if content.strip() else {}
            except json.JSONDecodeError as e:
                validation_errors.append(
                    f"{sub_context} (tool: {tool_name}): 'content' is not valid JSON. Error: {e}. Content: '{content}'"
                )
                return None, validation_errors
        else:
            validation_errors.append(
                f"{sub_context} (tool: {tool_name}): Expected JSON string or object for tool arguments, but got {type(content)}. Content: '{content}'"
            )
            return None, validation_errors

        # Validate the parsed content against the tool's input schema
        if tool_to_use.input_schema and parsed_content is not None:
            try:
                validate(instance=parsed_content, schema=tool_to_use.input_schema)
            except ValidationError as e:
                validation_errors.append(
                    f"{sub_context} (tool: {tool_name}): JSON arguments in 'content' do not match the tool's input schema. Error: {e.message}. Path: {'/'.join(map(str, e.path))}. Schema: {e.schema}. Args: {parsed_content}"
                )
                return None, validation_errors
            except Exception as e:  # Catch other potential validation errors
                validation_errors.append(
                    f"{sub_context} (tool: {tool_name}): Error validating arguments against tool schema. Error: {e}. Args: {parsed_content}"
                )
                return None, validation_errors

        return parsed_content, validation_errors

    def _validate_agent_task(self, content: Any, sub_context: str) -> List[str]:
        """Validates a subtask intended for agent execution."""
        validation_errors: List[str] = []
        if not isinstance(content, str) or not content.strip():
            validation_errors.append(
                f"{sub_context}: 'content' must be a non-empty string containing instructions when 'tool_name' is not provided, but got: '{content}' (type: {type(content)})."
            )
        return validation_errors

    def _process_subtask_schema(
        self, subtask_data: dict, sub_context: str
    ) -> tuple[Optional[str], List[str]]:
        """Processes and validates the output_schema for a subtask."""
        validation_errors: List[str] = []
        output_type: str = subtask_data.get("output_type", "string")
        current_schema_str: Any = subtask_data.get("output_schema")
        final_schema_str: Optional[str] = None
        schema_dict: Optional[dict] = None

        if is_binary_output_type(output_type):
            final_schema_str = json.dumps(FILE_POINTER_SCHEMA)
        else:
            try:
                if isinstance(current_schema_str, str) and current_schema_str.strip():
                    schema_dict = json.loads(current_schema_str)
                elif current_schema_str is None or (
                    isinstance(current_schema_str, str)
                    and not current_schema_str.strip()
                ):
                    # If schema is None or empty string, generate default based on type
                    schema_dict = json_schema_for_output_type(output_type)
                else:  # Invalid type for schema string
                    raise ValueError(
                        f"Output schema must be a JSON string or None, got {type(current_schema_str)}"
                    )

                # Apply defaults if schema_dict was successfully loaded or generated
                if schema_dict is not None:
                    schema_dict = self._ensure_additional_properties_false(schema_dict)
                    final_schema_str = json.dumps(schema_dict)
                # If schema_dict is still None here, it means default generation failed (unlikely with current json_schema_for_output_type)
                # or input was invalid type that didn't parse

            except (ValueError, json.JSONDecodeError) as e:
                validation_errors.append(
                    f"{sub_context}: Invalid output_schema provided: '{current_schema_str}'. Error: {e}. Using default for type '{output_type}'."
                )
                # Attempt to generate default schema as fallback
                try:
                    schema_dict = json_schema_for_output_type(output_type)
                    schema_dict = self._ensure_additional_properties_false(schema_dict)
                    final_schema_str = json.dumps(schema_dict)
                except Exception as default_e:
                    validation_errors.append(
                        f"{sub_context}: Failed to generate default schema for type '{output_type}' after error. Defaulting to string schema. Error: {default_e}"
                    )
                    # Final fallback: simple string schema
                    final_schema_str = json.dumps(
                        {"type": "string", "additionalProperties": False}
                    )

        return final_schema_str, validation_errors

    def _prepare_subtask_data(
        self,
        subtask_data: dict,
        final_schema_str: str,
        parsed_tool_content: Optional[dict],
        sub_context: str,
    ) -> tuple[Optional[dict], List[str]]:
        """Cleans paths and prepares the final data dictionary for SubTask creation."""
        validation_errors: List[str] = []
        processed_data = subtask_data.copy()  # Work on a copy

        try:
            processed_data["output_schema"] = (
                final_schema_str  # Already validated/generated
            )
            processed_data["output_file"] = self._clean_and_validate_path(
                processed_data.get("output_file", ""), f"{sub_context} output_file"
            )
            processed_data["input_files"] = [
                self._clean_and_validate_path(f, f"{sub_context} input_file '{f}'")
                for f in processed_data.get("input_files", [])
                if isinstance(f, str)  # Ensure items are strings
            ]
            processed_data["artifacts"] = [
                self._clean_and_validate_path(f, f"{sub_context} artifact '{f}'")
                for f in processed_data.get("artifacts", [])
                if isinstance(f, str)  # Ensure items are strings
            ]
            # Ensure tool_name is None if empty string or missing
            processed_data["tool_name"] = processed_data.get("tool_name") or None

            # Filter args based on SubTask model fields
            subtask_model_fields = SubTask.model_fields.keys()
            filtered_data = {
                k: v for k, v in processed_data.items() if k in subtask_model_fields
            }

            # Stringify content if it was a parsed JSON object (for tool args)
            if isinstance(parsed_tool_content, dict):
                # Ensure the original content key exists before assignment
                if "content" in filtered_data:
                    filtered_data["content"] = json.dumps(parsed_tool_content)
                else:
                    # This case might indicate an issue if content was expected but not provided/filtered
                    validation_errors.append(
                        f"{sub_context}: Content field missing after filtering, cannot stringify tool arguments."
                    )
                    return None, validation_errors
            elif isinstance(filtered_data.get("content"), str):
                pass  # Keep agent task content as string
            # else: # Handle cases where content might be missing or wrong type after filtering
            #    if filtered_data.get("tool_name"): # Tool tasks expect content
            #        validation_errors.append(f"{sub_context}: Missing or invalid 'content' for tool task.")
            #        return None, validation_errors
            # Agent tasks are validated earlier to ensure content is a string

            return filtered_data, validation_errors

        except ValueError as e:  # Catch path validation errors
            validation_errors.append(f"{sub_context}: Error processing paths: {e}")
            return None, validation_errors
        except Exception as e:  # Catch unexpected errors during preparation
            validation_errors.append(
                f"{sub_context}: Unexpected error preparing subtask data: {e}"
            )
            return None, validation_errors

    async def _process_single_subtask(
        self,
        subtask_data: dict,
        index: int,
        context_prefix: str,
        available_execution_tools: Dict[str, Tool],
    ) -> tuple[Optional[SubTask], List[str]]:
        """
        Processes and validates data for a single subtask by delegating steps.
        """
        sub_context = f"{context_prefix} subtask {index}"
        all_validation_errors: List[str] = []
        parsed_tool_content: Optional[dict] = (
            None  # To store parsed JSON for tool tasks
        )

        try:
            # --- Validate Tool Call vs Agent Instruction ---
            tool_name = subtask_data.get("tool_name")
            content = subtask_data.get("content")

            if tool_name:
                # --- Deterministic Tool Task Validation ---
                parsed_tool_content, tool_errors = self._validate_tool_task(
                    subtask_data,
                    tool_name,
                    content,
                    available_execution_tools,
                    sub_context,
                )
                all_validation_errors.extend(tool_errors)
                if (
                    parsed_tool_content is None and tool_errors
                ):  # Fatal error during tool validation
                    return None, all_validation_errors
            else:
                # --- Probabilistic Agent Task Validation ---
                agent_errors = self._validate_agent_task(content, sub_context)
                all_validation_errors.extend(agent_errors)
                if agent_errors:  # Fatal error during agent validation
                    return None, all_validation_errors

            # --- Process schema ---
            final_schema_str, schema_errors = self._process_subtask_schema(
                subtask_data, sub_context
            )
            all_validation_errors.extend(schema_errors)
            # Continue even if there were schema errors, as a default might be used.
            # Need final_schema_str for data preparation. If None, indicates a fatal schema issue.
            if final_schema_str is None:
                all_validation_errors.append(
                    f"{sub_context}: Fatal error processing output schema."
                )
                return None, all_validation_errors

            # --- Prepare data for SubTask creation (Paths, Filtering, Stringify Tool Args) ---
            filtered_data, preparation_errors = self._prepare_subtask_data(
                subtask_data, final_schema_str, parsed_tool_content, sub_context
            )
            all_validation_errors.extend(preparation_errors)
            if filtered_data is None:  # Fatal error during data preparation
                return None, all_validation_errors

            # --- Create SubTask object ---
            # Pydantic validation happens here
            subtask = SubTask(**filtered_data)
            # Return successful subtask and any *non-fatal* validation errors collected
            return subtask, all_validation_errors

        except (
            ValidationError
        ) as e:  # Catch Pydantic validation errors during SubTask(**filtered_data)
            all_validation_errors.append(
                f"{sub_context}: Invalid data for SubTask model: {e}"
            )
            return None, all_validation_errors
        except Exception as e:  # Catch any other unexpected errors
            all_validation_errors.append(
                f"{sub_context}: Unexpected error processing subtask: {e}\n{traceback.format_exc()}"
            )
            return None, all_validation_errors

    async def _process_subtask_list(
        self, raw_subtasks: list, context_prefix: str
    ) -> tuple[List[SubTask], List[str]]:
        """
        Processes a list of raw subtask data dictionaries using the helper method.
        """
        processed_subtasks: List[SubTask] = []
        all_validation_errors: List[str] = []
        # Build tool map once
        available_execution_tools: Dict[str, Tool] = {
            tool.name: tool for tool in self.execution_tools
        }

        for i, subtask_data in enumerate(raw_subtasks):
            sub_context = f"{context_prefix} subtask {i}"
            if not isinstance(subtask_data, dict):
                all_validation_errors.append(
                    f"{sub_context}: Expected subtask item to be a dict, but got {type(subtask_data)}. Data: {subtask_data}"
                )
                continue  # Skip this item

            # Call the helper to process this single subtask
            subtask, single_errors = await self._process_single_subtask(
                subtask_data, i, context_prefix, available_execution_tools
            )

            # Extend the list of errors collected (includes fatal and non-fatal)
            all_validation_errors.extend(single_errors)

            # Add the subtask ONLY if processing was successful (subtask is not None)
            if subtask:
                processed_subtasks.append(subtask)
            # If subtask is None, it means a fatal validation error occurred,
            # and the errors have already been added to all_validation_errors.

        return processed_subtasks, all_validation_errors

    async def _validate_structured_output_plan(
        self, task_data: dict, objective: str
    ) -> tuple[Optional[Task], List[str]]:
        """Validates the plan data received from structured output."""
        all_validation_errors: List[str] = []

        # Validate the subtasks first
        subtasks, subtask_validation_errors = await self._process_subtask_list(
            task_data.get("subtasks", []), "structured output"
        )
        all_validation_errors.extend(subtask_validation_errors)

        # If subtask processing had fatal errors, don't proceed to dependency check
        if not subtasks and task_data.get(
            "subtasks"
        ):  # Check if subtasks were provided but processing failed
            # Errors are already in all_validation_errors
            return None, all_validation_errors

        # Validate dependencies only if subtasks were processed successfully
        if subtasks:
            dependency_errors = self._validate_dependencies(subtasks)
            all_validation_errors.extend(dependency_errors)

        # If any fatal errors occurred anywhere, return None
        if any(
            err for err in all_validation_errors
        ):  # Check if there are any errors at all
            # Check specifically for fatal errors that would prevent Task creation
            # (e.g., invalid structure from _process_subtask_list, dependency errors)
            # For simplicity here, consider *any* validation error as potentially blocking.
            # A more nuanced check could differentiate warnings from fatal errors if needed.
            is_fatal = True  # Assume any error is fatal for now
            if is_fatal:
                return None, all_validation_errors

        # If validation passed (or only non-fatal errors occurred)
        return (
            Task(
                title=task_data.get("title", objective),
                subtasks=subtasks,  # Use the processed and validated subtasks
            ),
            all_validation_errors,
        )  # Return task and any non-fatal errors

    async def _create_task_with_structured_output(
        self, objective: str, max_retries: int = 3
    ) -> Task:
        """
        Create a task plan using structured output, with validation and retry logic.
        """
        create_task_tool = CreateTaskTool()
        response_schema: dict = create_task_tool.input_schema
        base_prompt: str = self._render_prompt(
            f"{PLAN_CREATION_TEMPLATE}\n{DEFAULT_AGENT_TASK_TEMPLATE}"
        )

        prompt: str = base_prompt  # Initial prompt
        current_retry: int = 0
        last_error: Optional[Exception] = None

        while current_retry < max_retries:
            attempt = current_retry + 1
            if self.console:
                self.console.print(
                    f"[yellow]Attempt {attempt}/{max_retries} for structured output...[/yellow]"
                )

            # Prepare messages for this attempt
            messages_for_attempt: List[Message] = [
                Message(role="system", content=self.system_prompt),
                Message(role="user", content=prompt),  # Use potentially updated prompt
            ]

            try:
                # 1. Generate Response
                message = await self.provider.generate_message(
                    messages=messages_for_attempt,
                    model=self.model,
                    response_format={
                        "type": "json_schema",
                        "json_schema": {
                            "name": "TaskPlan",
                            "schema": response_schema,
                            "strict": True,
                        },
                    },
                )

                if not isinstance(message.content, str) or not message.content.strip():
                    raise ValueError(
                        "LLM returned empty or non-string content for structured output."
                    )

                # 2. Parse JSON
                try:
                    task_data: dict = json.loads(message.content)
                except json.JSONDecodeError as json_err:
                    error_detail = f"Failed to decode JSON from LLM response: {json_err}.\nRaw Response:\n---\n{message.content}\n---"
                    if self.console:
                        self.console.print(
                            f"[red]JSON Decode Error:[/red]\n{error_detail}"
                        )
                    # Raise a ValueError containing the raw response for retry feedback
                    raise ValueError(error_detail) from json_err

                # 3. Validate Plan (Subtasks & Dependencies)
                validated_task, validation_errors = (
                    await self._validate_structured_output_plan(task_data, objective)
                )

                if validation_errors:
                    error_feedback = "\n".join(validation_errors)
                    if self.console:
                        self.console.print(
                            f"[red]Validation Errors (Structured):[/red]\n{error_feedback}"
                        )
                    # Raise ValueError with validation errors for retry feedback
                    raise ValueError(
                        f"Validation failed for structured output plan:\n{error_feedback}"
                    )

                # 4. Success
                if validated_task:
                    if self.console:
                        self.console.print(
                            "[green]Structured output plan created and validated successfully.[/green]"
                        )
                    return validated_task  # Success!
                else:
                    # Should not happen if validation_errors was empty, but handle defensively
                    raise ValueError(
                        "Plan validation reported no errors, but no task object was returned."
                    )

            except Exception as e:
                last_error = e
                current_retry += 1  # Increment retry count
                if current_retry < max_retries:
                    # Prepare prompt for the next retry, including the error message
                    prompt = f"{base_prompt}\n\nPREVIOUS ATTEMPT FAILED WITH ERROR (Attempt {attempt}/{max_retries}):\n{str(e)}\n\nPlease fix the errors and regenerate the full plan according to the schema."
                    if self.console:
                        self.console.print(
                            f"[yellow]Retrying structured output ({current_retry + 1}/{max_retries})...[/yellow]"
                        )
                    await asyncio.sleep(1)  # Optional delay before retry
                else:
                    # Raise the last error encountered after exhausting retries
                    raise ValueError(
                        f"Failed structured output after {max_retries} attempts."
                    ) from last_error

        # Should ideally not be reached if max_retries > 0
        raise ValueError(
            f"Failed to create task with structured output after {max_retries} attempts (unexpected exit). Last error: {str(last_error)}"
        ) from last_error

    async def _validate_and_build_task_from_tool_calls(
        self, tool_calls: List[ToolCall], history: List[Message]
    ) -> tuple[Optional[Task], List[str]]:
        """Processes 'create_task' tool calls, validates subtasks and dependencies."""
        all_subtasks: List[SubTask] = []
        all_validation_errors: List[str] = []
        task_title: str = self.objective  # Default title
        tool_responses_added: Set[str] = set()  # Track processed tool call IDs

        for tool_call in tool_calls:
            if tool_call.id in tool_responses_added:
                continue  # Already processed

            if tool_call.name == "create_task":
                context_prefix = f"processing tool call {tool_call.id}"
                # Extract title safely
                task_title = tool_call.args.get("title", task_title)

                # --- Process Subtasks using helper ---
                raw_subtasks_list = tool_call.args.get("subtasks", [])
                if not isinstance(raw_subtasks_list, list):
                    all_validation_errors.append(
                        f"{context_prefix}: 'subtasks' field must be a list, but got {type(raw_subtasks_list)}."
                    )
                    # Skip processing this call's subtasks if field is invalid, but record error
                    subtasks, validation_errors = [], []  # Ensure these are empty lists
                else:
                    # Use the updated _process_subtask_list
                    subtasks, validation_errors = await self._process_subtask_list(
                        raw_subtasks_list, context_prefix
                    )

                all_subtasks.extend(subtasks)
                all_validation_errors.extend(validation_errors)
                # --- End Subtask Processing ---

                # Add tool response message *after* processing its args
                response_content = "Task parameters received and processed."
                call_specific_errors = [
                    e for e in validation_errors if context_prefix in e
                ]
                if call_specific_errors:
                    response_content = f"Task parameters received, but validation errors occurred: {'; '.join(call_specific_errors)}"

                history.append(
                    Message(
                        role="tool", content=response_content, tool_call_id=tool_call.id
                    )
                )
                tool_responses_added.add(tool_call.id)  # Mark as processed

            else:
                # Handle unexpected tool calls
                error_msg = f"Unexpected tool call received: {tool_call.name}"
                all_validation_errors.append(error_msg)
                history.append(
                    Message(
                        role="tool",
                        content=f"Error: {error_msg}",
                        tool_call_id=tool_call.id,
                    )
                )
                tool_responses_added.add(tool_call.id)

        # --- Validate Dependencies *after* collecting all subtasks ---
        # Only run if there were subtasks and no fundamental structural errors earlier
        if all_subtasks and not any(
            "must be a list" in e for e in all_validation_errors
        ):
            dependency_errors = self._validate_dependencies(all_subtasks)
            all_validation_errors.extend(dependency_errors)
        # --- End Dependency Validation ---

        # Check if any errors occurred
        if all_validation_errors:
            # Raise ValueError to trigger retry logic in _generate_with_retry
            error_string = "\n".join(all_validation_errors)
            raise ValueError(f"Validation errors in created task:\n{error_string}")

        # Ensure we actually created subtasks if the call succeeded validation
        if not all_subtasks and any(tc.name == "create_task" for tc in tool_calls):
            # Check if create_task was called but resulted in no valid subtasks
            # This might happen if the subtasks list was empty or all items failed validation
            raise ValueError(
                "Task creation tool call processed, but resulted in zero valid subtasks."
            )

        # If validation passed and subtasks exist
        return (
            Task(title=task_title, subtasks=all_subtasks),
            all_validation_errors,
        )  # Return task and empty error list

    async def _process_tool_calls(
        self, message: Message, history: List[Message]
    ) -> Task:
        """
        Helper method to process tool calls, create task, and handle validation.
        Delegates validation logic to _validate_and_build_task_from_tool_calls.
        """
        if not message.tool_calls:
            raise ValueError(f"No tool calls found in the message: {message.content}")

        # Delegate the core processing and validation
        task, validation_errors = await self._validate_and_build_task_from_tool_calls(
            message.tool_calls, history
        )

        # _validate_and_build_task_from_tool_calls raises ValueError on failure,
        # so if we reach here, validation passed.
        if task is None:
            # This case should technically be handled by the exception in the helper,
            # but added for defensive programming.
            raise ValueError(
                "Task validation passed but task object is unexpectedly None."
            )

        return task

    async def _generate_with_retry(
        self, history: List[Message], tools: List[Tool], max_retries: int = 3
    ) -> tuple[Optional[Task], Optional[Message]]:
        """
        Generates response, processes tool calls with validation and retry logic.
        """
        current_retry: int = 0
        last_message: Optional[Message] = None

        while current_retry < max_retries:
            attempt = current_retry + 1
            # Generate response using current history
            message = await self.provider.generate_message(
                messages=history,
                model=self.model,
                tools=tools,
            )
            history.append(
                message
            )  # Add assistant's response to history *before* processing
            last_message = message

            if not message.tool_calls:
                # LLM didn't use the expected tool
                if tools and current_retry < max_retries - 1:
                    current_retry += 1
                    tool_names = ", ".join([t.name for t in tools])
                    retry_prompt = f"Please use one of the available tools ({tool_names}) to define the task based on the previous analysis and requirements."
                    history.append(Message(role="user", content=retry_prompt))
                    if self.console:
                        self.console.print(
                            f"[yellow]Retry {attempt}/{max_retries}: Asking LLM to use required tool(s).[/yellow]"
                        )
                    continue  # Go to next iteration
                else:
                    # Max retries reached without tool use
                    if self.console:
                        self.console.print(
                            f"[red]Failed after {max_retries} retries: LLM did not use the required tool(s). Last message: {self._format_message_content(message)}[/red]"
                        )
                    return None, last_message

            # Tool call exists, try to process it
            try:
                # Process the tool call(s). This adds 'tool' role messages to history
                # and raises ValueError on validation failure.
                task = await self._process_tool_calls(message, history)
                # If _process_tool_calls returns without error, success!
                if self.console:
                    self.console.print(
                        "[green]Tool call processed successfully.[/green]"
                    )
                return (
                    task,
                    last_message,
                )  # Return created task and the assistant message
            except ValueError as e:  # Catch validation errors from _process_tool_calls
                if self.console:
                    self.console.print(
                        f"[yellow]Validation Error (Retry {attempt}/{max_retries}):[/yellow]\n{str(e)}"
                    )
                if current_retry < max_retries - 1:
                    current_retry += 1
                    # Add a user message asking LLM to fix errors. The 'tool' message with error details
                    # should already be in history from _process_tool_calls failing.
                    retry_prompt = f"The previous attempt failed validation. Please review the errors detailed in the tool response and call the tool again correctly:\n{str(e)}"
                    history.append(Message(role="user", content=retry_prompt))
                    if self.console:
                        self.console.print(
                            f"[yellow]Retry {attempt + 1}/{max_retries}: Asking LLM to fix validation errors.[/yellow]"
                        )
                    # Optional: Add a small delay before retrying
                    # await asyncio.sleep(1)
                    continue  # Go to next iteration
                else:
                    # Max retries reached after validation errors
                    if self.console:
                        self.console.print(
                            f"[red]Failed after {max_retries} retries due to persistent validation errors.[/red]"
                        )
                    return (
                        None,
                        last_message,
                    )  # Return no task and the last assistant message

        # Should only be reached if max_retries is 0 or loop finishes unexpectedly
        return None, last_message

    async def create_task(self, context: ProcessingContext, objective: str) -> Task:
        """
        Creates or loads a task execution plan for the given objective.
        """
        # Consider adding logic here to load existing plan if desired (currently missing)
        # if await self._load_existing_plan():
        #     if self.console: self.console.print("[blue]Loaded existing task plan from tasks.yaml[/blue]")
        #     return self.task_plan # Type checker knows self.task_plan is TaskPlan now

        # If not loaded, create a new one
        task: Task = await self._create_task_for_objective(
            context, objective, max_retries=3
        )
        # Optionally save the newly created plan
        # self.task_plan = TaskPlan(objective=objective, tasks=[task]) # Wrap Task in TaskPlan
        # await self.save_task_plan()
        return task

    async def save_task_plan(self) -> None:
        """
        Save the current task plan to tasks.yaml.
        """
        if self.task_plan:
            task_dict: dict = self.task_plan.model_dump(
                exclude_none=True
            )  # Use exclude_none
            try:
                with open(self.tasks_file_path, "w") as f:
                    yaml.dump(
                        task_dict,
                        f,
                        indent=2,
                        sort_keys=False,
                        default_flow_style=False,
                    )  # Improve formatting
                if self.console:
                    self.console.print(
                        f"[cyan]Task plan saved to {self.tasks_file_path}[/cyan]"
                    )
            except IOError as e:
                if self.console:
                    self.console.print(
                        f"[red]Error saving task plan to {self.tasks_file_path}: {e}[/red]"
                    )
            except (
                Exception
            ) as e:  # Catch other potential errors (e.g., yaml serialization)
                if self.console:
                    self.console.print(
                        f"[red]Unexpected error saving task plan: {e}[/red]"
                    )

    def _get_execution_tools_info(self) -> str:
        """
        Get formatted string information about available execution tools.
        """
        if not self.execution_tools:
            return "No execution tools available"

        tools_info = "Available execution tools for subtasks:\n"
        for tool in self.execution_tools:
            # Add schema if available, keep it concise
            schema_info = ""
            if tool.input_schema and tool.input_schema.get("properties"):
                props = list(tool.input_schema["properties"].keys())
                req = tool.input_schema.get("required", [])
                prop_details = []
                for p in props:
                    is_req = " (required)" if p in req else ""
                    prop_details.append(f"{p}{is_req}")
                schema_info = f" | Args: {', '.join(prop_details)}"
            tools_info += f"- {tool.name}: {tool.description}{schema_info}\n"
        return tools_info.strip()  # Remove trailing newline

    def _get_retrieval_tools_info(self) -> str:
        """
        Get formatted string information about available retrieval tools.
        """
        if not self.retrieval_tools:
            return "No retrieval tools available for this phase."

        tools_info = "Available retrieval tools for information gathering:\n"
        for tool in self.retrieval_tools:
            # Add schema if available, keep it concise
            schema_info = ""
            if tool.input_schema and tool.input_schema.get("properties"):
                props = list(tool.input_schema["properties"].keys())
                req = tool.input_schema.get("required", [])
                prop_details = []
                for p in props:
                    is_req = " (required)" if p in req else ""
                    prop_details.append(f"{p}{is_req}")
                schema_info = f" | Args: {', '.join(prop_details)}"
            tools_info += f"- {tool.name}: {tool.description}{schema_info}\n"
        return tools_info.strip()  # Remove trailing newline

    def _get_input_files_info(self) -> str:
        """
        Get formatted string information about input files.
        """
        if not self.input_files:
            return "No input files available"

        input_files_info = "Input files:\n"
        for file_path in self.input_files:
            input_files_info += f"- {file_path}\n"
        return input_files_info.strip()  # Remove trailing newline

    def _ensure_additional_properties_false(self, schema: dict) -> dict:
        """
        Recursively ensures additionalProperties: false on objects, adds default items
        for arrays, and sets all object properties as required by default.
        """
        # Handle the current level if it's an object schema
        if isinstance(schema, dict) and schema.get("type") == "object":
            # Set additionalProperties if not explicitly defined
            schema.setdefault("additionalProperties", False)

            # Make all defined properties required if 'required' is not already present
            if (
                "properties" in schema
                and isinstance(schema["properties"], dict)
                and "required" not in schema
            ):
                prop_names = list(schema["properties"].keys())
                if prop_names:  # Only add 'required' if there are properties
                    schema["required"] = prop_names

        # Handle arrays - add a default items field if 'items' is missing
        elif (
            isinstance(schema, dict)
            and schema.get("type") == "array"
            and "items" not in schema
        ):
            schema["items"] = {"type": "string"}  # Default to string items

        # Recursively process nested schemas within properties
        if (
            isinstance(schema, dict)
            and "properties" in schema
            and isinstance(schema["properties"], dict)
        ):
            for prop_name, prop_schema in schema["properties"].items():
                # Ensure prop_schema is a dict before recursing
                if isinstance(prop_schema, dict):
                    schema["properties"][prop_name] = (
                        self._ensure_additional_properties_false(prop_schema)
                    )

        # Recursively process nested schemas within array items
        if isinstance(schema, dict) and "items" in schema:
            items_schema = schema["items"]
            if isinstance(items_schema, dict):
                schema["items"] = self._ensure_additional_properties_false(items_schema)
            elif isinstance(items_schema, list):  # Handle tuple schemas in 'items'
                schema["items"] = [
                    (
                        self._ensure_additional_properties_false(item)
                        if isinstance(item, dict)
                        else item
                    )
                    for item in items_schema
                ]

        # Recursively process anyOf, allOf, oneOf schemas
        for key in ["anyOf", "allOf", "oneOf"]:
            if (
                isinstance(schema, dict)
                and key in schema
                and isinstance(schema[key], list)
            ):
                schema[key] = [
                    (
                        self._ensure_additional_properties_false(item)
                        if isinstance(item, dict)
                        else item
                    )
                    for item in schema[key]
                ]

        return schema
