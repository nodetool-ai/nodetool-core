# HuggingFace Inference Endpoint Dockerfile for NodeTool
#
# This Dockerfile is optimized for HuggingFace Inference Endpoints.
# It creates a lightweight container that can handle inference requests
# through the HuggingFace custom handler interface.
#
# Build:
#   docker build -f Dockerfile.hf -t your-username/nodetool-hf:latest .
#
# Test locally:
#   docker run -p 8080:8080 -e HF_TOKEN=your_token your-username/nodetool-hf:latest

FROM python:3.11-slim AS base

# Environment variables
ENV DEBIAN_FRONTEND=noninteractive \
    SHELL=/bin/bash \
    LC_ALL=C.UTF-8 \
    LANG=C.UTF-8 \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    # Essential build tools
    build-essential \
    git \
    wget \
    curl \
    # Image processing
    libopencv-dev \
    libjpeg-dev \
    libpng-dev \
    # Audio/Video processing
    ffmpeg \
    libsndfile1 \
    # Other essentials
    libssl-dev \
    libffi-dev && \
    # Clean up
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Install Python dependencies
COPY --from=ghcr.io/astral-sh/uv:0.8.5 /uv /uvx /bin/

# Create virtual environment
RUN uv venv /app/venv
ENV PATH=/app/venv/bin:$PATH
ENV VIRTUAL_ENV=/app/venv

# Install Flask for the inference server
RUN uv pip install flask gunicorn

# Install nodetool-core and dependencies
ARG USE_LOCAL_REPO=0
COPY --chown=root:root . /tmp/nodetool-core

RUN if [ "$USE_LOCAL_REPO" = "1" ]; then \
        echo "Installing from local repository..." && \
        cd /tmp/nodetool-core && \
        uv pip install --no-cache-dir \
            --index-strategy unsafe-best-match \
            --index-url https://pypi.org/simple \
            --extra-index-url https://nodetool-ai.github.io/nodetool-registry/simple/ \
            -e . ; \
    else \
        echo "Installing from nodetool-registry..." && \
        uv pip install --no-cache-dir \
            --index-strategy unsafe-best-match \
            --index-url https://pypi.org/simple \
            --extra-index-url https://nodetool-ai.github.io/nodetool-registry/simple/ \
            nodetool-core nodetool-base ; \
    fi && \
    # Clean up
    rm -rf /tmp/nodetool-core && \
    rm -rf /root/.cache/pip && \
    rm -rf /tmp/*

# Copy handler script
COPY src/nodetool/deploy/hf_inference.py /app/handler.py

# Create model directory (required by HuggingFace)
RUN mkdir -p /app/model

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:${PORT:-8080}/health || exit 1

# Expose port (HuggingFace typically uses 8080)
EXPOSE 8080

# Environment variables for HuggingFace
ENV PORT=8080
ENV HF_MODEL_DIR=/app/model

# Run the handler using gunicorn for production
CMD ["gunicorn", "--bind", "0.0.0.0:8080", "--workers", "1", "--timeout", "120", "handler:create_app()"]
